Subject: [PATCH] Refactoring of FocusGuardian application
---
Index: src/detector.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/detector.py b/src/detector.py
new file mode 100644
--- /dev/null	(date 1771226306774)
+++ b/src/detector.py	(date 1771226306774)
@@ -0,0 +1,84 @@
+from typing import Tuple
+
+import cv2
+from mediapipe.tasks import python
+from mediapipe.tasks.python import vision
+
+from config import Config
+
+
+LEFT_EYE_IDXS = (33, 133, 160, 159, 158, 144, 145, 153)
+RIGHT_EYE_IDXS = (362, 263, 387, 386, 385, 373, 374, 380)
+NOSE_TIP_IDX = 1
+LEFT_EAR_IDX = 234
+RIGHT_EAR_IDX = 454
+CHIN_IDX = 152
+TOP_HEAD_IDX = 10
+
+
+def create_detector(model_path: str) -> vision.FaceLandmarker:
+    base_options = python.BaseOptions(model_asset_path=model_path)
+    options = vision.FaceLandmarkerOptions(
+        base_options=base_options,
+        num_faces=1,
+        output_face_blendshapes=True,
+        running_mode=vision.RunningMode.IMAGE,
+    )
+    return vision.FaceLandmarker.create_from_options(options)
+
+
+def get_eye_bbox(landmarks, indices, width: int, height: int) -> Tuple[int, int, int, int]:
+    x_coords = [int(landmarks[i].x * width) for i in indices]
+    y_coords = [int(landmarks[i].y * height) for i in indices]
+    min_x, max_x = min(x_coords), max(x_coords)
+    min_y, max_y = min(y_coords), max(y_coords)
+    return (min_x, min_y, max_x - min_x, max_y - min_y)
+
+
+def analyze_focus(landmarks, blendshapes, config: Config) -> Tuple[bool, str, Tuple[int, int, int]]:
+    bs_dict = {cat.category_name: cat.score for cat in blendshapes}
+
+    look_down_score = (
+        bs_dict.get("eyeLookDownLeft", 0) + bs_dict.get("eyeLookDownRight", 0)
+    ) / 2
+    look_up_score = (
+        bs_dict.get("eyeLookUpLeft", 0) + bs_dict.get("eyeLookUpRight", 0)
+    ) / 2
+    look_side_score = max(
+        bs_dict.get("eyeLookInLeft", 0),
+        bs_dict.get("eyeLookOutLeft", 0),
+        bs_dict.get("eyeLookInRight", 0),
+        bs_dict.get("eyeLookOutRight", 0),
+    )
+
+    nose_y = landmarks[NOSE_TIP_IDX].y
+    left_ear_y = landmarks[LEFT_EAR_IDX].y
+    right_ear_y = landmarks[RIGHT_EAR_IDX].y
+    chin_y = landmarks[CHIN_IDX].y
+    top_head_y = landmarks[TOP_HEAD_IDX].y
+
+    face_height = chin_y - top_head_y
+    ears_y_avg = (left_ear_y + right_ear_y) / 2
+    head_pitch_ratio = (nose_y - ears_y_avg) / face_height
+
+    if look_down_score > config.threshold_down:
+        status_text = f"LOOKING DOWN ({look_down_score:.2f})"
+    elif look_side_score > config.threshold_side:
+        status_text = f"LOOKING SIDE ({look_side_score:.2f})"
+    elif look_up_score > config.threshold_up:
+        status_text = f"LOOKING UP ({look_up_score:.2f})"
+    elif head_pitch_ratio > config.threshold_head_pitch:
+        status_text = f"HEAD DOWN ({head_pitch_ratio:.2f})"
+    else:
+        return True, "OK - you are focused", (0, 255, 0)
+
+    return False, status_text, (0, 0, 255)
+
+
+def draw_eye_boxes(frame, landmarks, color: Tuple[int, int, int]) -> None:
+    h, w, _ = frame.shape
+    lx, ly, lw, lh = get_eye_bbox(landmarks, LEFT_EYE_IDXS, w, h)
+    cv2.rectangle(frame, (lx, ly), (lx + lw, ly + lh), color, 2)
+
+    rx, ry, rw, rh = get_eye_bbox(landmarks, RIGHT_EYE_IDXS, w, h)
+    cv2.rectangle(frame, (rx, ry), (rx + rw, ry + rh), color, 2)
Index: src/penalty.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/penalty.py b/src/penalty.py
new file mode 100644
--- /dev/null	(date 1771226216658)
+++ b/src/penalty.py	(date 1771226216658)
@@ -0,0 +1,48 @@
+from typing import Optional, Tuple
+
+import cv2
+import pygame
+
+
+def init_audio(music_path: str) -> None:
+    pygame.mixer.init()
+    pygame.mixer.music.load(music_path)
+
+
+class PenaltyController:
+    def __init__(self, video_path: str, window_title: str, frame_size: Tuple[int, int]):
+        self.video_path = video_path
+        self.window_title = window_title
+        self.frame_size = frame_size
+        self._cap: Optional[cv2.VideoCapture] = None
+
+    def start(self) -> None:
+        if self._cap is None:
+            self._cap = cv2.VideoCapture(self.video_path)
+        if not pygame.mixer.music.get_busy():
+            pygame.mixer.music.play(-1)
+
+    def stop(self) -> None:
+        pygame.mixer.music.stop()
+        if self._cap:
+            self._cap.release()
+            self._cap = None
+        try:
+            cv2.destroyWindow(self.window_title)
+        except Exception:
+            pass
+
+    def read_frame(self):
+        if self._cap is None:
+            return None
+
+        ret, video_frame = self._cap.read()
+        if not ret:
+            self._cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
+            ret, video_frame = self._cap.read()
+
+        if not ret:
+            return None
+
+        return cv2.resize(video_frame, self.frame_size)
+
Index: src/app.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/app.py b/src/app.py
new file mode 100644
--- /dev/null	(date 1771226426979)
+++ b/src/app.py	(date 1771226426979)
@@ -0,0 +1,98 @@
+import time
+from pathlib import Path
+
+import cv2
+import mediapipe as mp
+import pygame
+
+from config import resolve_config
+from detector import analyze_focus, create_detector, draw_eye_boxes
+from penalty import PenaltyController, init_audio
+
+
+def main() -> None:
+    project_root = Path(__file__).resolve().parent.parent
+    config = resolve_config(project_root)
+
+    detector = create_detector(config.model_path)
+    cap = cv2.VideoCapture(0)
+    init_audio(config.music_path)
+    penalty = PenaltyController(
+        config.video_path,
+        config.penalty_window_title,
+        config.penalty_frame_size,
+    )
+
+    look_away_start = None
+    penalty_active = False
+
+    print("FocusGuardian is active...")
+
+    try:
+        while cap.isOpened():
+            success, frame = cap.read()
+            if not success:
+                break
+
+            frame = cv2.flip(frame, 1)
+
+            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
+            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
+            detection_result = detector.detect(mp_image)
+
+            is_looking_at_screen = False
+            status_text = "NO FACE!"
+            color = (0, 0, 255)
+
+            if detection_result.face_landmarks:
+                landmarks = detection_result.face_landmarks[0]
+                blendshapes = detection_result.face_blendshapes[0]
+                is_looking_at_screen, status_text, color = analyze_focus(
+                    landmarks, blendshapes, config
+                )
+                draw_eye_boxes(frame, landmarks, color)
+
+            if is_looking_at_screen:
+                look_away_start = None
+                if penalty_active:
+                    penalty_active = False
+                    penalty.stop()
+                    print("You cameback! Video stopped")
+            else:
+                if look_away_start is None:
+                    look_away_start = time.time()
+
+                elapsed = time.time() - look_away_start
+                status_text += f" | {round(elapsed, 1)}s"
+
+                if elapsed > config.look_away_limit:
+                    penalty_active = True
+
+            cv2.putText(
+                frame,
+                status_text,
+                (20, 50),
+                cv2.FONT_HERSHEY_SIMPLEX,
+                0.8,
+                color,
+                2,
+            )
+            cv2.imshow(config.window_title, frame)
+
+            if penalty_active:
+                penalty.start()
+                video_frame = penalty.read_frame()
+                if video_frame is not None:
+                    cv2.imshow(config.penalty_window_title, video_frame)
+
+            if cv2.waitKey(5) & 0xFF == 27:
+                break
+    finally:
+        cap.release()
+        penalty.stop()
+        pygame.mixer.quit()
+        cv2.destroyAllWindows()
+
+
+if __name__ == "__main__":
+    main()
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
--- a/README.md	(revision a3428a72b92009601929b922fad061c06ae1d35e)
+++ b/README.md	(date 1771226426982)
@@ -45,9 +45,9 @@
 
 ## üöÄ How to Use
 
-Simply run the main script:
+Simply run the app entrypoint:
 ```bash
-python src/main.py
+python src/app.py
 ```
 
 - **OK - you are focused**: Everything is fine (Green boxes).
@@ -56,7 +56,7 @@
 
 ## ‚öôÔ∏è Configuration
 
-You can fine-tune the sensitivity in `src/main.py` to match your camera setup:
+You can fine-tune the sensitivity in `src/config.py` to match your camera setup:
 
 | Parameter | Default | Description |
 | :--- | :--- | :--- |
Index: src/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/config.py b/src/config.py
new file mode 100644
--- /dev/null	(date 1771226426977)
+++ b/src/config.py	(date 1771226426977)
@@ -0,0 +1,28 @@
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Tuple
+
+
+@dataclass(frozen=True)
+class Config:
+    model_path: str = "data/face_landmarker.task"
+    video_path: str = "data/video.mp4"
+    music_path: str = "data/video.wav"
+    look_away_limit: float = 0.5
+    threshold_down: float = 0.5
+    threshold_side: float = 0.6
+    threshold_up: float = 0.5
+    threshold_head_pitch: float = 0.30
+    window_title: str = "Anty-Doom-Scrolling"
+    penalty_window_title: str = "VIDEO PENALTY"
+    penalty_frame_size: Tuple[int, int] = (800, 600)
+
+
+def resolve_config(base_dir: Path) -> Config:
+    resolved_base = base_dir.resolve()
+    data_dir = resolved_base / "data"
+    return Config(
+        model_path=str(data_dir / "face_landmarker.task"),
+        video_path=str(data_dir / "video.mp4"),
+        music_path=str(data_dir / "video.wav"),
+    )
Index: src/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/main.py b/src/main.py
--- a/src/main.py	(revision a3428a72b92009601929b922fad061c06ae1d35e)
+++ b/src/main.py	(date 1771226216655)
@@ -1,173 +1,5 @@
-import time
-
-import cv2
-import mediapipe as mp
-import pygame
-from mediapipe.tasks import python
-from mediapipe.tasks.python import vision
-
-MODEL_PATH = "data/face_landmarker.task"
-VIDEO_PATH = "data/video.mp4"
-MUSIC_PATH = "data/video.wav"
-LOOK_AWAY_LIMIT = 0.5
-THRESHOLD_DOWN = 0.5
-THRESHOLD_SIDE = 0.6
-THRESHOLD_UP = 0.5
-THRESHOLD_HEAD_PITCH = 0.30
-
-base_options = python.BaseOptions(model_asset_path=MODEL_PATH)
-options = vision.FaceLandmarkerOptions(
-    base_options=base_options,
-    num_faces=1,
-    output_face_blendshapes=True,
-    running_mode=vision.RunningMode.IMAGE,
-)
-detector = vision.FaceLandmarker.create_from_options(options)
-
-cap = cv2.VideoCapture(0)
-penalty_cap = None
-look_away_start = None
-penalty_active = False
-pygame.mixer.init()
-pygame.mixer.music.load(MUSIC_PATH)
-LEFT_EYE_IDXS = [33, 133, 160, 159, 158, 144, 145, 153]
-RIGHT_EYE_IDXS = [362, 263, 387, 386, 385, 373, 374, 380]
-NOSE_TIP_IDX = 1
-LEFT_EAR_IDX = 234
-RIGHT_EAR_IDX = 454
-CHIN_IDX = 152
-TOP_HEAD_IDX = 10
-
-
-def get_eye_bbox(landmarks, indices, width, height):
-    x_coords = [int(landmarks[i].x * width) for i in indices]
-    y_coords = [int(landmarks[i].y * height) for i in indices]
-    min_x, max_x = min(x_coords), max(x_coords)
-    min_y, max_y = min(y_coords), max(y_coords)
-    return (min_x, min_y, max_x - min_x, max_y - min_y)
-
-
-print("FocusGuardian is active...")
-
-while cap.isOpened():
-    success, frame = cap.read()
-    if not success:
-        break
-
-    frame = cv2.flip(frame, 1)
-    h, w, _ = frame.shape
-
-    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
-    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
-
-    detection_result = detector.detect(mp_image)
-
-    is_looking_at_screen = False
-    color = (0, 0, 255)
-
-    if detection_result.face_landmarks:
-        landmarks = detection_result.face_landmarks[0]
-        blendshapes = detection_result.face_blendshapes[0]
-        bs_dict = {cat.category_name: cat.score for cat in blendshapes}
-
-        look_down_score = (
-            bs_dict.get("eyeLookDownLeft", 0) + bs_dict.get("eyeLookDownRight", 0)
-        ) / 2
-        look_up_score = (
-            bs_dict.get("eyeLookUpLeft", 0) + bs_dict.get("eyeLookUpRight", 0)
-        ) / 2
-        look_side_score = max(
-            bs_dict.get("eyeLookInLeft", 0),
-            bs_dict.get("eyeLookOutLeft", 0),
-            bs_dict.get("eyeLookInRight", 0),
-            bs_dict.get("eyeLookOutRight", 0),
-        )
-
-        nose_y = landmarks[NOSE_TIP_IDX].y
-        left_ear_y = landmarks[LEFT_EAR_IDX].y
-        right_ear_y = landmarks[RIGHT_EAR_IDX].y
-        chin_y = landmarks[CHIN_IDX].y
-        top_head_y = landmarks[TOP_HEAD_IDX].y
-
-        face_height = chin_y - top_head_y
-        ears_y_avg = (left_ear_y + right_ear_y) / 2
-        head_pitch_ratio = (nose_y - ears_y_avg) / face_height
-
-        if look_down_score > THRESHOLD_DOWN:
-            status_text = f"LOOKING DOWN ({look_down_score:.2f})"
-        elif look_side_score > THRESHOLD_SIDE:
-            status_text = f"LOOKING SIDE ({look_side_score:.2f})"
-        elif look_up_score > THRESHOLD_UP:
-            status_text = f"LOOKING UP ({look_up_score:.2f})"
-        elif head_pitch_ratio > THRESHOLD_HEAD_PITCH:
-            status_text = f"HEAD DOWN ({head_pitch_ratio:.2f})"
-        else:
-            is_looking_at_screen = True
-            status_text = "OK - you are focused"
-            color = (0, 255, 0)
-
-        lx, ly, lw, lh = get_eye_bbox(landmarks, LEFT_EYE_IDXS, w, h)
-        cv2.rectangle(frame, (lx, ly), (lx + lw, ly + lh), color, 2)
-
-        rx, ry, rw, rh = get_eye_bbox(landmarks, RIGHT_EYE_IDXS, w, h)
-        cv2.rectangle(frame, (rx, ry), (rx + rw, ry + rh), color, 2)
-
-        # ny = int(nose_y * h)
-        # nx = int(landmarks[NOSE_TIP_IDX].x * w)
-        # ey = int(ears_y_avg * h)
-        # cv2.line(frame, (0, ey), (w, ey), (255, 255, 0), 1) # ear line
-        # cv2.circle(frame, (nx, ny), 5, (255, 0, 255), -1)   # nose dot
-
-    else:
-        status_text = "NO FACE!"
+from app import main
 
-    if is_looking_at_screen:
-        look_away_start = None
-        pygame.mixer.music.stop()
-        if penalty_active:
-            penalty_active = False
-            if penalty_cap:
-                penalty_cap.release()
-                penalty_cap = None
-            try:
-                cv2.destroyWindow("VIDEO PENALTY")
-            except:
-                pass
-            print("You cameback! Video stopped")
-    else:
-        if look_away_start is None:
-            look_away_start = time.time()
 
-        elapsed = time.time() - look_away_start
-        status_text += f" | {round(elapsed, 1)}s"
-
-        if elapsed > LOOK_AWAY_LIMIT:
-            penalty_active = True
-
-    cv2.putText(frame, status_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
-    cv2.imshow("Anty-Doom-Scrolling", frame)
-
-    if penalty_active:
-        if penalty_cap is None:
-            penalty_cap = cv2.VideoCapture(VIDEO_PATH)
-            if not pygame.mixer.music.get_busy():
-                pygame.mixer.music.play(-1)
-
-        ret, video_frame = penalty_cap.read()
-
-        if not ret:
-            penalty_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
-            ret, video_frame = penalty_cap.read()
-
-        if ret:
-            video_frame = cv2.resize(video_frame, (800, 600))
-            cv2.imshow("VIDEO PENALTY", video_frame)
-
-    if cv2.waitKey(5) & 0xFF == 27:
-        break
-
-cap.release()
-if penalty_cap:
-    penalty_cap.release()
-pygame.mixer.quit()
-cv2.destroyAllWindows()
+if __name__ == "__main__":
+    main()
